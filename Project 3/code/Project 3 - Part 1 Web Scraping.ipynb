{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87629b27",
   "metadata": {},
   "source": [
    "# Project 3 - Part 1 Web Scraping\n",
    "by: Nah Wei Jie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f37883",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "This project is broken down into 3 different parts, each with it's accompanying notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f4381",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "- Web scraping  \n",
    "\n",
    "**Part 2** \n",
    "\n",
    "- Exploratory Data Analysis\n",
    "- Data Cleaning\n",
    "- Visualizations\n",
    "\n",
    "**Part 3**\n",
    "- Pre-processing\n",
    "- Model Fit and Testing\n",
    "- Model Iteration\n",
    "- Model Evaluation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03ad14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34792fc",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5b93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Processing imports\n",
    "import pandas as pd # Version 1.2.4\n",
    "\n",
    "#Scraping Imports\n",
    "import requests # Version 2.25.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58ad35",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f24c8",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1b8c2",
   "metadata": {},
   "source": [
    "The data was retrieved from two different subreddits, namely [**r/FanFiction**](https://www.reddit.com/r/FanFiction) and [**r/LifeAdvice**](https://www.reddit.com/r/LifeAdvice). As Reddit's .json format is conveniently structured like a Python dictionary, by relying on [**Pushshift's API**](https://github.com/pushshift/api) and the ```requests``` library, we are able to easily extract the information from Reddit, retrieving 100 posts in each request.\n",
    "\n",
    "To give us enough data in our corpus, the loop was called 10 times for each subreddit, although the requests resulted in about 990+ posts for the FanFiction subreddit rather than 1000.  \n",
    "\n",
    "For the purposes of this project, we scraped about ~2000 posts with 20 ```HTTP``` requests made to Reddit via the API. As the API has a query limit of 100 posts, it is considered [**good practice**](https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c) to use a random timeout before the start of each loop (see line ```14/15``` in code block ```2```). This is to avoid visits to the API/Reddit too frequently which may result in us getting blocked/banned as well as to alleviate the traffic for normal visitors of Reddit.\n",
    "\n",
    "For building larger datasets or scraping across a higher number of different subreddits in future, scraping with consideration for the target API/website may be of even higher importance, especially when we are doing it programmatically.\n",
    "\n",
    "\n",
    "As we have already scraped through both subreddits before, some code blocks below has been commented to prevent overwriting of existing data which may affect our analysis. The output shown below the code block is the result of the instance I ran on my end. The purpose of inlcuding it here is to show it's functionality as well as to show that the code works.\n",
    "\n",
    "Viewers may uncomment the code block and attempt to run it, however do be warned that it might take quite a while to finish depending on the network and computing resources of the viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39dac7c",
   "metadata": {},
   "source": [
    "### Scraping from FanFiction subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f93635",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request iteration: 1\n",
      " Status code: 200\n",
      "Number of posts scraped:  100\n",
      "Request iteration: 2\n",
      " Status code: 200\n",
      "Number of posts scraped:  200\n",
      "Request iteration: 3\n",
      " Status code: 200\n",
      "Number of posts scraped:  300\n",
      "Request iteration: 4\n",
      " Status code: 200\n",
      "Number of posts scraped:  400\n",
      "Request iteration: 5\n",
      " Status code: 200\n",
      "Number of posts scraped:  499\n",
      "Request iteration: 6\n",
      " Status code: 200\n",
      "Number of posts scraped:  599\n",
      "Request iteration: 7\n",
      " Status code: 200\n",
      "Number of posts scraped:  699\n",
      "Request iteration: 8\n",
      " Status code: 200\n",
      "Number of posts scraped:  799\n",
      "Request iteration: 9\n",
      " Status code: 200\n",
      "Number of posts scraped:  898\n",
      "Request iteration: 10\n",
      " Status code: 200\n",
      "Number of posts scraped:  998\n",
      "**End of scrape**\n"
     ]
    }
   ],
   "source": [
    "# # Initializing our URL, params and empty lists to capture posts\n",
    "# url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "# posts_ff = [] # Initialize an empty list to facilitate storing of results\n",
    "\n",
    "# # Below we set the following parameters for Pushshift's API\n",
    "# params = {\n",
    "#     'subreddit': 'FanFiction', # We use this to inform the API which subreddit to scrape from\n",
    "#     'size': 100, # We use this to inform the API how many posts to retrieve from Reddit (Hard Limited at 100)\n",
    "#     'before': None, # We use this to provide an 'index' for the API to know where to continue from where it has previously stopped\n",
    "# }\n",
    "\n",
    "# # Below we use a for loop to execute the the command 10 times to get ~ 1000 posts\n",
    "# for i in range(10): # Loop 10 times\n",
    "#     sleep_dur = max(random.gauss(3,2),2) # Generate a random sleep interval with minimal value being 2 seconds as sleep_dur\n",
    "#     time.sleep(sleep_dur) # Sleep for duration to prevent too frequent request\n",
    "#     res = requests.get(url, params) # Request data from Reddit\n",
    "#     if res.status_code != 200:   #Print error if request fails\n",
    "#         print('Status error', res.status_code)\n",
    "#         break\n",
    "#     else:\n",
    "#         print(f'Request iteration: {i+1}\\n Status code: {res.status_code}') # Print iteration count and corresponding status code\n",
    "#     data = res.json() # Store results in data variable\n",
    "#     posts = data['data'] # Store results into list\n",
    "#     if len(posts) > 0: # Condition to check if our results are returning nothing\n",
    "#         newbefore = posts[-1]['created_utc']\n",
    "#         params['before'] = newbefore # Replace the value of 'created_utc' of last post in current request iteration as the new 'before' value in our parameters\n",
    "#         posts_ff.extend(posts) # Add the 100 results of the current request loop into the posts list for FanFiction\n",
    "#         print('Number of posts scraped: ', len(posts_ff))\n",
    "#     else:\n",
    "#         print('Request did not fetch any results')\n",
    "# print('**End of scrape**')        \n",
    "# ## Click on the ... bubble to see the output of each iterative scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12091089",
   "metadata": {},
   "source": [
    "We will now repeat the same steps for the LifeAdvice subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc304f",
   "metadata": {},
   "source": [
    "### Scraping from LifeAdvice subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fce9f0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request iteration: 1\n",
      " Status code: 200\n",
      "Number of posts scraped:  100\n",
      "Request iteration: 2\n",
      " Status code: 200\n",
      "Number of posts scraped:  200\n",
      "Request iteration: 3\n",
      " Status code: 200\n",
      "Number of posts scraped:  300\n",
      "Request iteration: 4\n",
      " Status code: 200\n",
      "Number of posts scraped:  400\n",
      "Request iteration: 5\n",
      " Status code: 200\n",
      "Number of posts scraped:  500\n",
      "Request iteration: 6\n",
      " Status code: 200\n",
      "Number of posts scraped:  600\n",
      "Request iteration: 7\n",
      " Status code: 200\n",
      "Number of posts scraped:  700\n",
      "Request iteration: 8\n",
      " Status code: 200\n",
      "Number of posts scraped:  800\n",
      "Request iteration: 9\n",
      " Status code: 200\n",
      "Number of posts scraped:  900\n",
      "Request iteration: 10\n",
      " Status code: 200\n",
      "Number of posts scraped:  1000\n",
      "**End of scrape**\n"
     ]
    }
   ],
   "source": [
    "# # Initializing our URL, params and empty lists to capture posts\n",
    "# url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "# posts_la = [] # Initialize an empty list to facilitate storing of results\n",
    "\n",
    "# # Below we set the following parameters for Pushshift's API\n",
    "# params = {\n",
    "#     'subreddit': 'LifeAdvice', # We use this to inform the API which subreddit to scrape from\n",
    "#     'size': 100, # We use this to inform the API how many posts to retrieve from Reddit (Hard Limited at 100)\n",
    "#     'before': None, # We use this to provide an 'index' for the API to know where to continue from where it has previously stopped\n",
    "# }\n",
    "\n",
    "# # Below we use a for loop to execute the the command 10 times to get ~ 1000 posts\n",
    "# for i in range(10): # Loop 10 times\n",
    "#     sleep_dur = max(random.gauss(3,2),2) # Generate a random sleep interval with minimal value being 2 seconds as sleep_dur\n",
    "#     time.sleep(sleep_dur) # Sleep for duration to prevent too frequent request\n",
    "#     res = requests.get(url, params) # Request data from Reddit\n",
    "#     if res.status_code != 200:   #Print error if request fails\n",
    "#         print('Status error', res.status_code)\n",
    "#         break\n",
    "#     else:\n",
    "#         print(f'Request iteration: {i+1}\\n Status code: {res.status_code}') # Print iteration count and corresponding status code\n",
    "#     data = res.json() # Store results in data variable\n",
    "#     posts = data['data'] # Store results into list\n",
    "#     if len(posts) > 0: # Condition to check if our results are returning nothing\n",
    "#         newbefore = posts[-1]['created_utc']\n",
    "#         params['before'] = newbefore # Replace the value of 'created_utc' of last post in current request iteration as the new 'before' value in our parameters\n",
    "#         posts_la.extend(posts) # Add the 100 results of the current request loop into the posts list for FanFiction\n",
    "#         print('Number of posts scraped: ', len(posts_la))\n",
    "#     else:\n",
    "#         print('Request did not fetch any results')\n",
    "# print('**End of scrape**')\n",
    "# ## Click on the ... bubble to see output of each iterative scrape of 100 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26bda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading our posts from each subreddit as dataframes\n",
    "df_fanfic = pd.DataFrame(posts_ff)\n",
    "df_lifeadv = pd.DataFrame(posts_la)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1af2ef",
   "metadata": {},
   "source": [
    "### Preliminary Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0de5620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 76)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of rows, columns for fanfic dataframe\n",
    "df_fanfic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a95e6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 69)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of rows, columns for lifeadvice dataframe\n",
    "df_lifeadv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51878bd4",
   "metadata": {},
   "source": [
    "We have slightly more entries from our lifeadvice subreddit and we can see that the number of columns is different as well. As the different subreddits may have different rules and different features for their posts, these differences betwee nthe two is expected. In the next notebook, we will want to identify those columns that are interesting to us and ensure that both datasets have this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce143dcd",
   "metadata": {},
   "source": [
    "To keep to the scope of this notebook we will save the dataframes as they are now and do the  detailed cleaning and processing steps in the subsequent notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efef4a4",
   "metadata": {},
   "source": [
    "### Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d2f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code block has been commented to prevent overwriting the .csv files in case the scraping code block has been ran by the viewer\n",
    "\n",
    "## Saving the dataframes into their corresponding named .csv files\n",
    "# df_fanfic.to_csv('../datasets/fanfic.csv', index = False)\n",
    "# df_lifeadv.to_csv('../datasets/lifeadv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46b821",
   "metadata": {},
   "source": [
    "--- End of Notebook ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e225f8b",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
